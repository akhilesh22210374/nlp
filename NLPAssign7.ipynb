{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwvxUF9B2VGjcwpVxLuEgy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prafull009/NLP/blob/main/NLPAssign7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **N Gram Auto-Completion Implementation 1**"
      ],
      "metadata": {
        "id": "pFWwX79So5PR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "s-yJXOz2dI7a"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import random\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams, trigrams\n",
        "from nltk.corpus import reuters, gutenberg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('reuters')\n",
        "nltk.download('gutenberg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWQfzYpydu4X",
        "outputId": "70943aa2-c0a4-41a3-c76c-fb9208793825"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gutenberg = gutenberg.raw()\n",
        "\n",
        "# corpus = \" \".join(gutenberg).lower()  # Convert to lowercase\n",
        "corpus = gutenberg.raw().lower()  # Convert to lowercase\n",
        "corpus = re.sub(r\"[^a-z\\s]\", \"\", corpus)  # Remove punctuation"
      ],
      "metadata": {
        "id": "pni2qJb0dySq"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(corpus)"
      ],
      "metadata": {
        "id": "slbkPbLaeIl2"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 2  # Change to 3 for trigrams, 4 for 4-grams, etc.\n",
        "ngrams_list = list(ngrams(tokens, n))\n",
        "\n",
        "# Create frequency dictionary\n",
        "ngram_freq = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "for gram in ngrams_list:\n",
        "    prev_word, next_word = gram[0], gram[1]\n",
        "    ngram_freq[prev_word][next_word] += 1\n",
        "\n",
        "# Convert counts to probabilities\n",
        "ngram_prob = {}\n",
        "for prev_word in ngram_freq:\n",
        "    total_count = sum(ngram_freq[prev_word].values())\n",
        "    ngram_prob[prev_word] = {word: count / total_count for word, count in ngram_freq[prev_word].items()}"
      ],
      "metadata": {
        "id": "xQ3kW4kTg9rY"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(prev_word, top_k=3):\n",
        "    if prev_word in ngram_prob:\n",
        "        sorted_predictions = sorted(ngram_prob[prev_word].items(), key=lambda x: x[1], reverse=True)\n",
        "        return [word for word, prob in sorted_predictions[:top_k]]\n",
        "    else:\n",
        "        return [\"No prediction available\"]"
      ],
      "metadata": {
        "id": "aW4DeoEDhsuK"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"economics\"\n",
        "predictions = predict_next_word(input_text)\n",
        "print(f\"Predictions for '{input_text}': {predictions}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tks2gVgliMUp",
        "outputId": "af649343-46cd-443b-b921-000eda56c327"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions for 'economics': ['at']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(input_text, top_k=1):\n",
        "    input_text = preprocess_text(input_text)  # Preprocess input\n",
        "    input_seq = \" \".join(input_text[-(n-1):])  # Get last n-1 words\n",
        "\n",
        "    if input_seq in ngram_prob:\n",
        "        sorted_predictions = sorted(ngram_prob[input_seq].items(), key=lambda x: x[1], reverse=True)\n",
        "        return sorted_predictions[0][0] if sorted_predictions else \"No prediction available\"\n",
        "    else:\n",
        "        return \"No prediction available\"\n",
        "\n",
        "def generate_text(seed_text, num_predictions=5):\n",
        "    generated_text = seed_text\n",
        "\n",
        "    for _ in range(num_predictions):\n",
        "        next_word = predict_next_word(generated_text)\n",
        "        if next_word == \"No prediction available\":\n",
        "            break  # Stop if no valid prediction\n",
        "        generated_text += \" \" + next_word  # Append the predicted word\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Example usage\n",
        "seed_text = \"economic\"\n",
        "generated_sequence = generate_text(seed_text, num_predictions=6)\n",
        "\n",
        "print(\"Generated Text:\", generated_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Hg6O-2Bk9vH",
        "outputId": "b00bced4-362b-4880-e9ab-769ac8962244"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: economic reasons for the lord and the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **N Gram Auto-Completion Implementation 2**"
      ],
      "metadata": {
        "id": "qsXk8s5FotHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "import string\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "class MyNGramModel:\n",
        "    def __init__(self, n=3, diversity_factor=0.6, temperature=1.2):\n",
        "        \"\"\"\n",
        "        n: Size of the n-gram (e.g., 2 for bigram, 3 for trigram)\n",
        "        diversity_factor: Reduces probability of repeated words\n",
        "        temperature: Higher values make predictions more random\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.diversity_factor = diversity_factor\n",
        "        self.temperature = temperature\n",
        "        self.model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "    def train(self, text):\n",
        "        tokens = nltk.word_tokenize(text.lower())\n",
        "        tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "        if len(tokens) < self.n:\n",
        "            print(\"Not enough words to generate n-grams.\")\n",
        "            return\n",
        "\n",
        "        # Count n-gram occurrences\n",
        "        for i in range(len(tokens) - (self.n - 1)):\n",
        "            ngram = tuple(tokens[i:i + self.n - 1])\n",
        "            next_word = tokens[i + self.n - 1]\n",
        "            self.model[ngram][next_word] += 1  # Count occurrences\n",
        "\n",
        "        # Convert counts to probabilities (apply temperature scaling)\n",
        "        for ngram in self.model:\n",
        "            total_count = float(sum(self.model[ngram].values()))\n",
        "            for word in self.model[ngram]:\n",
        "                self.model[ngram][word] = (self.model[ngram][word] / total_count) ** (1 / self.temperature)\n",
        "\n",
        "    def predict_next(self, prefix, previous_words):\n",
        "        \"\"\"\n",
        "        Predicts the next word using temperature-scaled probabilities.\n",
        "        \"\"\"\n",
        "        prefix_tokens = nltk.word_tokenize(prefix.lower())\n",
        "        if len(prefix_tokens) < (self.n - 1):\n",
        "            return None\n",
        "\n",
        "        prefix_ngram = tuple(prefix_tokens[-(self.n - 1):])\n",
        "        next_word_probs = self.model.get(prefix_ngram, {})\n",
        "\n",
        "        if not next_word_probs:\n",
        "            return None\n",
        "\n",
        "        # Apply diversity factor (reduce probability of repeated words)\n",
        "        adjusted_probs = {}\n",
        "        for word, prob in next_word_probs.items():\n",
        "            adjusted_probs[word] = prob * (self.diversity_factor if word in previous_words else 1)\n",
        "\n",
        "        # Normalize probabilities\n",
        "        total_adjusted = sum(adjusted_probs.values())\n",
        "        if total_adjusted == 0:\n",
        "            return None\n",
        "        for word in adjusted_probs:\n",
        "            adjusted_probs[word] /= total_adjusted\n",
        "\n",
        "        # Weighted random choice\n",
        "        words, probabilities = zip(*adjusted_probs.items())\n",
        "        return random.choices(words, probabilities)[0]\n",
        "\n",
        "    def autocomplete(self, prefix, num_suggestions=10):\n",
        "        \"\"\"\n",
        "        Generates a sequence of words with better randomness and reduced repetition.\n",
        "        \"\"\"\n",
        "        suggestions = []\n",
        "        previous_words = set()\n",
        "        for _ in range(num_suggestions):\n",
        "            next_word = self.predict_next(prefix, previous_words)\n",
        "            if not next_word:\n",
        "                break\n",
        "\n",
        "            # Stop early if next word is repetitive\n",
        "            if len(suggestions) > 3 and next_word in suggestions[-3:]:\n",
        "                break\n",
        "\n",
        "            prefix = f\"{prefix} {next_word}\"\n",
        "            suggestions.append(prefix)\n",
        "            previous_words.add(next_word)\n",
        "\n",
        "        return suggestions\n",
        "\n",
        "corpus_text = gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "# Train the N-gram model (Trigram Model)\n",
        "ngram_model = MyNGramModel()\n",
        "ngram_model.train(corpus_text)\n",
        "\n",
        "    # Get auto-completion suggestions\n",
        "prefix = input(\"Enter a prefix for auto-completion: \").strip()\n",
        "suggestions = ngram_model.autocomplete(prefix)\n",
        "\n",
        "print(\"\\nAuto-complete Suggestions:\")\n",
        "for suggestion in suggestions:\n",
        "  print(suggestion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv4la97nmYew",
        "outputId": "1eaa831b-237b-4ef9-adbf-3a55823642df"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a prefix for auto-completion: He was a\n",
            "\n",
            "Auto-complete Suggestions:\n",
            "He was a strange\n",
            "He was a strange thing\n",
            "He was a strange thing love\n",
            "He was a strange thing love is\n",
            "He was a strange thing love is he\n",
            "He was a strange thing love is he to\n",
            "He was a strange thing love is he to be\n",
            "He was a strange thing love is he to be collected\n",
            "He was a strange thing love is he to be collected she\n",
            "He was a strange thing love is he to be collected she was\n"
          ]
        }
      ]
    }
  ]
}